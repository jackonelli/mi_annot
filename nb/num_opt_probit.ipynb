{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84345ca",
   "metadata": {},
   "source": [
    "# Numerical opt. of $alpha_{1:n}$ for probit class\n",
    "\n",
    "Assume a model with a fairly informative prior:\n",
    "$$\n",
    " \\Theta \\sim N(\\Theta; \\mu_{\\theta}, \\sigma^2_{\\Theta}) \\\\\n",
    " B \\sim U(B; a_{B}, b_{B})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isclose, log, log2\n",
    "from functools import partial\n",
    "from scipy.stats import norm, bernoulli\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.uniform import Uniform\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (12,5)\n",
    "\n",
    "\n",
    "def p(z, alpha_i):\n",
    "    \"\"\"Likelihood p(Ỹ_i = ỹ_i | Theta = theta, X_i = x_i),\n",
    "    with z = theta^T x_i - beta\n",
    "    \"\"\"\n",
    "    return (2*alpha_i - 1)*Normal(loc=0, scale=1).cdf(z) + 1-alpha_i\n",
    "\n",
    "def h(p):\n",
    "    \"\"\"Entropy of a Bernoulli variable\"\"\"\n",
    "    # Set h = 0 in the limit p -> 0, and p -> 1\n",
    "    entropy = torch.empty(p.shape)\n",
    "    one_lim = torch.isclose(p, torch.ones(p.shape))\n",
    "    zero_lim = torch.isclose(p, torch.zeros(p.shape))\n",
    "    limit_inds = torch.logical_or(one_lim, zero_lim)\n",
    "    entropy[limit_inds] = 0\n",
    "    # For the rest, compute it the usual way.\n",
    "    ok_inds = torch.logical_not(limit_inds)\n",
    "    ok_p = p[ok_inds]\n",
    "    entropy[ok_inds] = -ok_p*torch.log2(ok_p) - (1-ok_p)*torch.log2(1-ok_p)\n",
    "    return entropy\n",
    "\n",
    "a_B = -1\n",
    "b_B = 1\n",
    "mu_theta = 1e6\n",
    "sigma_sq_theta = 0.1\n",
    "theta = Normal(loc=mu_theta, scale=sigma_sq_theta).sample()\n",
    "beta = Uniform(a_B, b_B).sample()\n",
    "num_samples = 10\n",
    "# xs = 2*torch.Tensor(norm.rvs(size=num_samples))\n",
    "x_range = torch.arange(xs.min(), xs.max(), 1/num_samples)\n",
    "y_tildes = torch.Tensor(bernoulli.rvs(p=p(theta*(xs-beta), alpha_i=1.0)))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_range, p(theta*x_range - beta, alpha_i = 0.8), label=\"p(ỹ_i | x_i, t, b)\")\n",
    "#ax.plot(xs, torch.zeros(xs.size()), 'b.')\n",
    "ax.plot(xs, y_tildes, 'b.')\n",
    "ax.set_xlabel(\"$x_i$\");\n",
    "ax.set_ylabel(\"$p$\");\n",
    "\n",
    "\n",
    "\n",
    "def entropy_of_avg(x_seq, alpha_seq, param_sampling):\n",
    "    \"\"\"H[Ỹ_1:n | X_1:n]\n",
    "    Compute the probability for every ỹ in {0,1}^n\n",
    "    and evaluate the bernoulli entropy.\n",
    "    \"\"\"\n",
    "    elements = torch.combinations(torch.Tensor([0,1]), r=x_seq.size(0), with_replacement=True)\n",
    "    thetas, betas = param_sampling()\n",
    "    entropy = torch.tensor(0.0)\n",
    "    ps = torch.tensor(0.0)\n",
    "    for y_tilde_seq in elements:\n",
    "        for params in zip(thetas, betas):\n",
    "            ps += p_y_tilde_seq_given_params(y_tilde_seq, x_seq, params, alpha_seq)\n",
    "        prob = ps / thetas.size(0)\n",
    "        print(prob, entropy)\n",
    "        entropy -= prob * torch.log2(prob)\n",
    "    return entropy / elements.size(0)\n",
    "\n",
    "def p_y_tilde_seq_given_params(y_tilde_seq, x_seq, params, alpha_seq):\n",
    "    \"\"\"Likelihood p(Ỹ_1:n = ỹ_1:n | Theta = theta, X_1:n = x_1:n)\n",
    "    \n",
    "    Args:\n",
    "        y_tilde_seq (np.array): Seq. of Bernoulli variables (n,): {0, 1}^n\n",
    "        x_seq (np.array): Seq. of np.arrays (n, D_x)\n",
    "        theta (np.array): Vector (D_x,)\n",
    "        alpha_seq (np.array): Seq of prec. values (n,): [0.5, 1]^n \n",
    "    \"\"\"\n",
    "    theta, beta = params\n",
    "    true_inds = y_tilde_seq == 1\n",
    "    false_inds = y_tilde_seq == 0\n",
    "    z = theta * (x_seq - beta)\n",
    "    alpha_seq[true_inds]\n",
    "    true_prod = p(z[true_inds], alpha_seq[true_inds])\n",
    "    false_prod = 1 - p(z[false_inds], alpha_seq[false_inds])\n",
    "    return true_prod.prod() * false_prod.prod()\n",
    "\n",
    "xs = torch.tensor([-2, -1/2, 0, 1/2])\n",
    "alphas = torch.tensor([1/2, 1/2, 1, 1/2])\n",
    "sampler = partial(param_sampling, mu_theta, sigma_sq_theta, a_B, b_B, num_mc_samples)\n",
    "avg_entropy(xs, alphas, sampler)\n",
    "\n",
    "# torch.combinations_with_replacement(torch.Tensor([0,1]), r=4, with_replacement=False)\n",
    "# help(torch.combinations)\n",
    "p_y_tilde_seq_given_params(torch.tensor([0, 0, 1, 1]), xs, (torch.tensor(1e6), torch.tensor(0.0)), alphas)\n",
    "\n",
    "# torch.combinations(torch.Tensor([0,1]), r=4, with_replacement=True)\n",
    "entropy_of_avg(xs, alphas, sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00937bfa",
   "metadata": {},
   "source": [
    "## Opt problem\n",
    "maximise\n",
    "$$\n",
    "L = \n",
    "\\mathcal{H}[\\tilde{Y}_{1:n} \\vert X_{1:n}]\n",
    "- \\mathbb{E}_{\\Theta \\sim p(\\Theta \\vert X_{1:n})} \\left[\\mathcal{H}[\\tilde{Y}_{1:n} \\vert \\theta, x_{1:n}] \\right] \\\\\n",
    " = \\mathcal{H}(\\int p_{\\alpha_{1:n}}(Ỹ_{1:n} = ỹ_{1:n} | x_{1:n}, \\theta, \\beta) N(\\theta; \\mu_{\\Theta}, \\sigma^2_{\\theta}) U(\\beta; a_B, b_B) d \\theta d \\beta) \\\\\n",
    " - \\int \\sum_i  h ((2\\alpha_i - 1) \\Phi(\\theta^{\\top}(x_i - \\beta)) + 1 - \\alpha_i) N(\\theta; \\mu_{\\Theta}, \\sigma^2_{\\theta}) U(\\beta; a_B, b_B) d \\theta d \\beta,\n",
    "$$\n",
    "with respect to $\\alpha_i \\in [0.5, 1.0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfac6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_y_tilde_seq_given_params(y_tilde_seq, x_seq, params, alpha_seq):\n",
    "    \"\"\"Likelihood p(Ỹ_1:n = ỹ_1:n | Theta = theta, X_1:n = x_1:n)\n",
    "    \n",
    "    Args:\n",
    "        y_tilde_seq (np.array): Seq. of Bernoulli variables (n,): {0, 1}^n\n",
    "        x_seq (np.array): Seq. of np.arrays (n, D_x)\n",
    "        theta (np.array): Vector (D_x,)\n",
    "        alpha_seq (np.array): Seq of prec. values (n,): [0.5, 1]^n \n",
    "    \"\"\"\n",
    "    theta, beta = params\n",
    "    true_inds = y_tilde_seq == 1\n",
    "    false_inds = y_tilde_seq == 0\n",
    "    z = theta * (x_seq - beta)\n",
    "    alpha_seq[true_inds]\n",
    "    true_prod = p(z[true_inds], alpha_seq[true_inds])\n",
    "    false_prod = 1 - p(z[false_inds], alpha_seq[false_inds])\n",
    "    return true_prod.prod() * false_prod.prod()\n",
    "\n",
    "def param_sampling(mu_theta, sigma_sq_theta, a_B, b_B, num_mc_samples):\n",
    "    theta = Normal(loc=mu_theta, scale=sigma_sq_theta).sample(sample_shape=(num_mc_samples,))\n",
    "    beta = Uniform(a_B, b_B).sample(sample_shape=(num_mc_samples,))\n",
    "    return (theta, beta)\n",
    "\n",
    "def entropy_of_avg(x_seq, alpha_seq, param_sampling):\n",
    "    \"\"\"H[Ỹ_1:n | X_1:n]\n",
    "    Compute the probability for every ỹ in {0,1}^n\n",
    "    and evaluate the bernoulli entropy.\n",
    "    \"\"\"\n",
    "    elements = torch.combinations(torch.Tensor([0,1]), r=x_seq.size(0), with_replacement=True)\n",
    "    thetas, betas = param_sampling()\n",
    "    entropy = torch.tensor(0.0)\n",
    "    ps = torch.tensor(0.0)\n",
    "    for y_tilde_seq in elements:\n",
    "        for params in zip(thetas, betas):\n",
    "            ps += p_y_tilde_seq_given_params(y_tilde_seq, x_seq, params, alpha_seq)\n",
    "        prob = ps / x_seq.size(0)\n",
    "        entropy -= prob * torch.log2(prob)\n",
    "    return entropy\n",
    "\n",
    "def avg_entropy(x_seq, alpha_seq, param_sampling):\n",
    "    \"\"\"E_p(theta) H[Ỹ_1:n | Theta, Beta, X_1:n]\"\"\"\n",
    "    thetas, betas = param_sampling()\n",
    "    \n",
    "    for params in zip(thetas, betas):\n",
    "        theta, beta = params\n",
    "        zs = theta * (xs - beta)\n",
    "        ps = p(zs, alpha_seq)\n",
    "        entropies = h(ps)\n",
    "    return entropies.mean()\n",
    "\n",
    "num_mc_samples = 100\n",
    "alphas = 0.8*torch.ones(xs.shape, requires_grad=True)\n",
    "sampler = partial(param_sampling, mu_theta, sigma_sq_theta, a_B, b_B, num_mc_samples)\n",
    "L = entropy_of_avg(xs, alphas, sampler) - avg_entropy(xs, alphas, sampler)\n",
    "torch.autograd.grad(L, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def alpha_mapping(zs):\n",
    "    \"\"\"Map unbounded proxy variable to [1/2, 1]\"\"\"\n",
    "    return 1/2 + 1/2*torch.sigmoid(zs)\n",
    "\n",
    "class MiOpt(nn.Module):\n",
    "    \"\"\"Custom Pytorch model for gradient optimization.\n",
    "    \"\"\"\n",
    "    def __init__(self, xs, sampling_fn):\n",
    "        super().__init__()\n",
    "        # proxy params for alphas with full support\n",
    "        weights = torch.distributions.Uniform(-10, 10).sample(xs.size())\n",
    "        # weights = 1 * torch.ones(xs.size())\n",
    "        self.weights = nn.Parameter(weights)\n",
    "        self._sampler = sampling_fn\n",
    "        self.xs = xs\n",
    "\n",
    "    def forward(self, _x):\n",
    "        alphas = alpha_mapping(self.weights)\n",
    "        L_1 = entropy_of_avg(self.xs, alphas, self._sampler)\n",
    "        #print(\"L_1\", L_1)\n",
    "        L_2 = avg_entropy(self.xs, alphas, self._sampler)\n",
    "        #print(\"L_2\", L_2)\n",
    "        return L_1 - L_2\n",
    "    \n",
    "def training_loop(model, optimizer, num_epochs):\n",
    "    \"Training loop for torch model.\"\n",
    "    mis = []\n",
    "    for i in range(num_epochs):\n",
    "        mi = model(None)\n",
    "        neg_mi = - mi\n",
    "        neg_mi.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        mis.append(mi.item()) \n",
    "        print(f\"Epoch: {i}, MI: {mi.item()}\")\n",
    "    return mis\n",
    "\n",
    "num_mc_samples = 100\n",
    "sampling_fn = partial(param_sampling, mu_theta, sigma_sq_theta, a_B, b_B, num_mc_samples)\n",
    "m = MiOpt(xs, sampling_fn)\n",
    "# Instantiate optimizer\n",
    "opt = torch.optim.Adam(m.parameters(), lr=0.01)\n",
    "losses = training_loop(m, opt, num_epochs = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a393a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zs = m.weights\n",
    "alphas = alpha_mapping(zs).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_range, p(theta*(x_range - beta), alpha_i = 1.0), label=\"p(ỹ_i | x_i, t, b)\")\n",
    "#ax.plot(xs, torch.zeros(xs.size()), 'b.')\n",
    "ax.plot(xs, alphas, 'b.')\n",
    "ax.set_xlabel(\"$x_i$\");\n",
    "ax.set_ylabel(\"$p$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis = []\n",
    "h_avgs = []\n",
    "avg_hs = []\n",
    "a_range = torch.linspace(0.5, 1.0, 10)\n",
    "for a in a_range:\n",
    "    # print(a)\n",
    "    alphas = a*torch.ones(xs.shape)\n",
    "    h_avg = entropy_of_avg(xs, alphas, sampler)\n",
    "    avg_h = avg_entropy(xs, alphas, sampler)\n",
    "    mi = h_avg - avg_h\n",
    "    mis.append(mi)\n",
    "    h_avgs.append(h_avg)\n",
    "    avg_hs.append(avg_h)\n",
    "mis = torch.tensor(mis)\n",
    "h_avgs = torch.tensor(h_avgs)\n",
    "avg_hs = torch.tensor(avg_hs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(a_range, mis, label=\"MI\")\n",
    "ax.plot(a_range, avg_hs, label=\"$E_{p(\\\\theta, \\\\beta)} H[Ỹ_{1:n} | \\\\theta, \\\\beta, X_{1:n}]$\")\n",
    "ax.plot(a_range, h_avgs, label=\"$H[Ỹ_{1:n}| X_{1:n}]$\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"$\\\\alpha$\");\n",
    "ax.set_ylabel(\"$MI$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf730807",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_mapping(0.1*torch.ones(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54257bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs, batch size, number of training data and learning rate\n",
    "\n",
    "num_epochs = 20 \n",
    "lr = 0.01\n",
    "\n",
    "zs = 0.8*torch.ones(xs.size(), requires_grad=True)\n",
    "for epoch in range(num_epochs):\n",
    "    costs = []\n",
    "    alphas = alpha_mapping(zs)\n",
    "    L_1 = entropy_of_avg(xs, alphas, sampler)\n",
    "    L_2 = avg_entropy(xs, alphas, sampler)\n",
    "   \n",
    "    L = - L_2\n",
    "    L.backward()\n",
    "    a = alphas.sum()\n",
    "    print(\"alphas\", a, a.grad)\n",
    "    print(\"L_1\", L_1, L_1.grad)\n",
    "    print(\"L_2\", L_2, L_2.grad)\n",
    "    print(L.grad)\n",
    "    costs.append(L.item())\n",
    "    with torch.no_grad():\n",
    "        alphas += alphas.grad * lr\n",
    "        alphas.grad.zero_()\n",
    "            \n",
    "    # Computing and printing the average loss in the current epoch\n",
    "    print('Epoch: {} Cost: {}'.format(epoch, L.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d781d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from copy import copy\n",
    "\n",
    "n = 1000\n",
    "noise = torch.Tensor(np.random.normal(0, 0.02, size=n))\n",
    "x = torch.arange(n)\n",
    "a, k, b = 0.7, .01, 0.2\n",
    "y = a * np.exp(-k * x) + b + noise\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"Custom Pytorch model for gradient optimization.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # initialize weights with random numbers\n",
    "        weights = torch.distributions.Uniform(0, 0.1).sample((3,))\n",
    "        # make weights torch parameters\n",
    "        self.weights = nn.Parameter(weights)        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Implement function to be optimised. In this case, an exponential decay\n",
    "        function (a + exp(-k * X) + b),\n",
    "        \"\"\"\n",
    "        a, k, b = self.weights\n",
    "        return a * torch.exp(-k * X) + b\n",
    "    \n",
    "def training_loop(model, optimizer, n=1000):\n",
    "    \"Training loop for torch model.\"\n",
    "    losses = []\n",
    "    for i in range(n):\n",
    "        preds = model(x)\n",
    "        loss = F.mse_loss(preds, y).sqrt()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())  \n",
    "    return losses\n",
    "\n",
    "# instantiate model\n",
    "m = Model()\n",
    "# Instantiate optimizer\n",
    "opt = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "losses = training_loop(m, opt)\n",
    "#plt.figure(figsize=(14, 7))\n",
    "plt.plot(losses)\n",
    "print(m.weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
