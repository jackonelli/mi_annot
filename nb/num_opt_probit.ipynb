{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84345ca",
   "metadata": {},
   "source": [
    "# Numerical opt. of $\\alpha_{1:n}$ for probit class\n",
    "\n",
    "Assume a model with a fairly informative prior:\n",
    "$$\n",
    " \\Theta \\sim N(\\Theta; \\mu_{\\theta}, \\sigma^2_{\\Theta}) \\\\\n",
    " B \\sim U(B; a_{B}, b_{B})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from functools import partial\n",
    "from scipy.stats import norm, bernoulli\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.uniform import Uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.probit.probit import (\n",
    "    p,\n",
    "    h,\n",
    "    p_y_tilde_seq_given_params,\n",
    "    entropy_of_avg,\n",
    "    avg_entropy,\n",
    "    sample_joint_normal_uniform\n",
    ")\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9dacd",
   "metadata": {},
   "source": [
    "# Examples with few samples\n",
    "\n",
    "Let $x_{1:n} = [-2, -1/2, 0, 1/2]$.\n",
    "\n",
    "$$\n",
    "B \\sim U(a_B, b_B)\\\\\n",
    "\\Theta \\sim(\\mu_{\\Theta}, \\sigma^2_{\\Theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0cf864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter prior\n",
    "a_B = -1\n",
    "b_B = 1\n",
    "mu_theta = 1e6\n",
    "sigma_sq_theta = 0.1\n",
    "theta = Normal(loc=mu_theta, scale=sigma_sq_theta).sample()\n",
    "beta = Uniform(a_B, b_B).sample()\n",
    "num_mc_samples = 10\n",
    "\n",
    "# Specific few samples example\n",
    "xs = torch.tensor([-2, -1/2, 0, 1/2])\n",
    "alphas = torch.tensor([1/2, 1/2, 1, 1/2])\n",
    "alphas = 1/2 * torch.ones(xs.size())\n",
    "sampler = partial(sample_joint_normal_uniform, mu_theta, sigma_sq_theta, a_B, b_B, num_mc_samples)\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.plot(xs, torch.zeros(xs.size()), 'bX', label=\"$x_{1:n}$\")\n",
    "#ax.plot([a_B, b_B], torch.zeros((2,)), 'r|', label=\"$\\\\beta$-intervall\")\n",
    "#ax.plot(xs, alphas, 'b.', label=\"$\\\\alpha_{1:n}$\")\n",
    "#ax.legend()\n",
    "\n",
    "#p_y_tilde_seq_given_params(torch.tensor([0, 0, 1, 1]), xs, (torch.tensor(1e6), torch.tensor(0.0)), alphas)\n",
    "#avg_entropy(xs, alphas, sampler)\n",
    "# print(binary_labels_combination(4))\n",
    "entropy_of_avg(xs, alphas, sampler)\n",
    "y_tilde_seq = torch.zeros(xs.size())\n",
    "y_tilde_seq = torch.tensor([1, 0, 0, 0])\n",
    "# p_y_tilde_seq_given_params(y_tilde_seq, xs, (1e6, 0.5), alphas)\n",
    "xs = torch.tensor([-2, -1/2, 0, 1/2])\n",
    "alphas = torch.tensor([1/2, 1/2, 1, 1/2])\n",
    "alphas = 0.55*torch.ones(xs.shape)\n",
    "avg_entropy(xs, alphas, sampler)\n",
    "entropy_of_avg(xs, alphas, sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00937bfa",
   "metadata": {},
   "source": [
    "## Opt problem\n",
    "maximise\n",
    "$$\n",
    "L = \n",
    "\\mathcal{H}[\\tilde{Y}_{1:n} \\vert X_{1:n}]\n",
    "- \\mathbb{E}_{\\Theta \\sim p(\\Theta \\vert X_{1:n})} \\left[\\mathcal{H}[\\tilde{Y}_{1:n} \\vert \\theta, x_{1:n}] \\right] \\\\\n",
    " = \\mathcal{H}(\\int p_{\\alpha_{1:n}}(Ỹ_{1:n} = ỹ_{1:n} | x_{1:n}, \\theta, \\beta) N(\\theta; \\mu_{\\Theta}, \\sigma^2_{\\theta}) U(\\beta; a_B, b_B) d \\theta d \\beta) \\\\\n",
    " - \\int \\sum_i  h ((2\\alpha_i - 1) \\Phi(\\theta^{\\top}(x_i - \\beta)) + 1 - \\alpha_i) N(\\theta; \\mu_{\\Theta}, \\sigma^2_{\\theta}) U(\\beta; a_B, b_B) d \\theta d \\beta,\n",
    "$$\n",
    "with respect to $\\alpha_i \\in [0.5, 1.0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfac6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from scipy.stats import norm, bernoulli\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.uniform import Uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.probit.probit import (\n",
    "    entropy_of_avg,\n",
    "    avg_entropy,\n",
    "    sample_joint_normal_uniform\n",
    ")\n",
    "from src.probit.opt import alpha_mapping\n",
    "\n",
    "\n",
    "a_B = -1\n",
    "b_B = 1\n",
    "mu_theta = 1e6\n",
    "sigma_sq_theta = 0.1\n",
    "theta = Normal(loc=mu_theta, scale=sigma_sq_theta).sample()\n",
    "beta = Uniform(a_B, b_B).sample()\n",
    "num_mc_samples = 1000\n",
    "\n",
    "xs = torch.tensor([-2, -1/2, 0, 1/2])\n",
    "# weights = torch.distributions.Uniform(-10, 10).sample(xs.size)\n",
    "weights = torch.ones(xs.size(), requires_grad=True)\n",
    "alphas = alpha_mapping(weights)\n",
    "sampling_fn = partial(sample_joint_normal_uniform, mu_theta, sigma_sq_theta, a_B, b_B, num_mc_samples)\n",
    "L = entropy_of_avg(xs, alphas, sampling_fn) - avg_entropy(xs, alphas, sampling_fn)\n",
    "torch.autograd.grad(L, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.probit.opt import alpha_mapping, inverse_alpha_mapping\n",
    "\n",
    "alpha = torch.distributions.Uniform(1 / 2, 1).sample()\n",
    "mapped_alpha = alpha_mapping(inverse_alpha_mapping(alpha))\n",
    "a = torch.allclose(alpha, mapped_alpha)\n",
    "alpha, mapped_alpha\n",
    "\n",
    "#inverse_alpha_mapping(alpha)\n",
    "alpha = 0.62\n",
    "1 - 1 / (2 * (alpha - 1 / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from functools import partial\n",
    "from scipy.stats import norm, bernoulli\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.uniform import Uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.probit.probit import (\n",
    "    entropy_of_avg,\n",
    "    avg_entropy,\n",
    "    sample_joint_normal_uniform\n",
    ")\n",
    "from src.probit.opt import MiOpt, linear_cost, alpha_mapping\n",
    "\n",
    "def training_loop(model, optimizer, num_epochs):\n",
    "    \"Training loop for torch model.\"\n",
    "    mis = []\n",
    "    for i in range(1, num_epochs + 1):\n",
    "        mi = model.compute_mi()\n",
    "        opt_objective = - mi + model.constraint()\n",
    "        opt_objective.backward()\n",
    "        optimizer.step()\n",
    "        params = next(model.parameters())\n",
    "        grads = [p.grad for p in model.parameters()]\n",
    "        print(\"grads: \", grads)\n",
    "        # print(\"grad elemt\", params.grad)\n",
    "        optimizer.zero_grad()\n",
    "        mis.append(mi.item()) \n",
    "        print(f\"Epoch: {i}, MI: {mi.item()}, alpha sum: {alpha_mapping(model.opt_param).sum()}\")\n",
    "    return mis\n",
    "\n",
    "a_B = -1\n",
    "b_B = 1\n",
    "mu_theta = 1e6\n",
    "sigma_sq_theta = 0.1\n",
    "theta = Normal(loc=mu_theta, scale=sigma_sq_theta).sample()\n",
    "beta = Uniform(a_B, b_B).sample()\n",
    "num_mc_samples = 100\n",
    "xs = torch.tensor([-2, -1/2, 0, 1/2])\n",
    "alphas = torch.tensor([1/2, 1/2, 1, 1/2])\n",
    "alphas = 0.55*torch.ones(xs.shape)\n",
    "# alphas = torch.tensor([0.55, 0.55, 0.95, 0.55])\n",
    "\n",
    "sampling_fn = partial(sample_joint_normal_uniform, mu_theta, sigma_sq_theta, a_B, b_B, num_mc_samples)\n",
    "m = MiOpt(xs, alphas, cost_fn=linear_cost, budget = 3, sampling_fn=sampling_fn)\n",
    "# Instantiate optimizer\n",
    "opt = torch.optim.SGD(m.parameters(), lr=10)\n",
    "losses = training_loop(m, opt, num_epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.alphas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54257bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs, batch size, number of training data and learning rate\n",
    "\n",
    "num_epochs = 20 \n",
    "lr = 0.01\n",
    "\n",
    "zs = 0.8*torch.ones(xs.size(), requires_grad=True)\n",
    "for epoch in range(num_epochs):\n",
    "    costs = []\n",
    "    alphas = alpha_mapping(zs)\n",
    "    L_1 = entropy_of_avg(xs, alphas, sampler)\n",
    "    L_2 = avg_entropy(xs, alphas, sampler)\n",
    "   \n",
    "    L = - L_2\n",
    "    L.backward()\n",
    "    a = alphas.sum()\n",
    "    print(\"alphas\", a, a.grad)\n",
    "    print(\"L_1\", L_1, L_1.grad)\n",
    "    print(\"L_2\", L_2, L_2.grad)\n",
    "    print(L.grad)\n",
    "    costs.append(L.item())\n",
    "    with torch.no_grad():\n",
    "        alphas += alphas.grad * lr\n",
    "        alphas.grad.zero_()\n",
    "            \n",
    "    # Computing and printing the average loss in the current epoch\n",
    "    print('Epoch: {} Cost: {}'.format(epoch, L.item()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
